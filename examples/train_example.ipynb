{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the mrpc dataset as an example dataset. It contains paraphrases and non-paraphrases. We will filter it and use only the paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('glue', 'mrpc')\n",
    "raw_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the splits separately and look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_train = raw_datasets['train']\n",
    "raw_datasets_val = raw_datasets['validation']\n",
    "raw_datasets_test = raw_datasets['test']\n",
    "raw_datasets_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly label=1 means that the sentences are paraphrases. Let's filter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = raw_datasets_train.filter(lambda x: x['label']==1)\n",
    "ds_val = raw_datasets_val.filter(lambda x: x['label']==1)\n",
    "ds_test = raw_datasets_test.filter(lambda x: x['label']==1)\n",
    "len(ds_train), len(ds_val), len(ds_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems legit. That's a little over half the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, if we want to use only a small subset of the data we can do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = ds_train.select(range(200))\n",
    "# ds_val = ds_val.select(range(40))\n",
    "# ds_test = ds_test.select(range(100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'google/mt5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI: [here](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#processing-the-data) it says to specify `tokenizer.src_lang` and `tokenizer.tgt_lang` for multilingual models/tokeinzers, but this tokenizer does not have these properties."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the tokenizer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ds_train[0]['sentence1']\n",
    "s2 = ds_train[0]['sentence2']\n",
    "print(s1)\n",
    "print(s2)\n",
    "inputs = tokenizer(s1, text_target=s2)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a preprocess function that turns a dataset item into a form that the model can use for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find out what a reasonable `max_len` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_lengths = list(map(lambda x: max(len(x['sentence1']), len(x['sentence2'])), ds_train))\n",
    "val_lengths = list(map(lambda x: max(len(x['sentence1']), len(x['sentence2'])), ds_train))\n",
    "test_lengths = list(map(lambda x: max(len(x['sentence1']), len(x['sentence2'])), ds_train))\n",
    "\n",
    "plt.hist(train_lengths + val_lengths + test_lengths, 100)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the y axis here is characters and not tokens. With 128 (for `max_len`) we are on the safe side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "# the prefix has to (dynamically) be adjusted depending on the language or when training multilingually (I think).\n",
    "prefix = 'paraphrase: '\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix+s1 for s1 in examples['sentence1']]\n",
    "    targets = examples['sentence2']\n",
    "    # most likely there will be nothing to truncate, but we still add it\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the preprocessing function to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_train = ds_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds_train.column_names\n",
    ")\n",
    "tokenized_ds_val = ds_val.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds_val.column_names\n",
    ")\n",
    "tokenized_ds_test = ds_test.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds_test.column_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the model and a Datacollator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'll skip the example usage of the datacollator, check it out [here](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#data-collation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's continue with metrics. We will use Parascore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parascore import ParaScorer\n",
    "\n",
    "scorer = ParaScorer(lang='en', model_type='bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly go over how Parascore is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = [\"A young person is skating.\", \"I like sports.\", \"He catches the ball.\", \"That's very interesting!\"]\n",
    "sources = [\"There's a child on a skateboard.\", \"I like to relax.\", \"good morning, everyone!\", \"I find this interesting.\"]\n",
    "score = scorer.free_score(cands, sources)\n",
    "float(score[-1].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here's the `compute_metrics` function (mostly copied from [here](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#metrics)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    parascore = scorer.free_score(decoded_preds, decoded_labels)\n",
    "    return {'parascore': float(parascore[-1].mean())}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir='.',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_ds_train,\n",
    "    eval_dataset=tokenized_ds_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpproject",
   "language": "python",
   "name": "nlpproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
