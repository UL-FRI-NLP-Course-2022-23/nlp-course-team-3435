{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the mrpc dataset as an example dataset. It contains paraphrases and non-paraphrases. We will filter it and use only the paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('glue', 'mrpc')\n",
    "raw_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the splits separately and look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_train = raw_datasets['train']\n",
    "raw_datasets_val = raw_datasets['validation']\n",
    "raw_datasets_test = raw_datasets['test']\n",
    "raw_datasets_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly label=1 means that the sentences are paraphrases. Let's filter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = raw_datasets_train.filter(lambda x: x['label']==1)\n",
    "ds_val = raw_datasets_val.filter(lambda x: x['label']==1)\n",
    "ds_test = raw_datasets_test.filter(lambda x: x['label']==1)\n",
    "len(ds_train), len(ds_val), len(ds_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems legit. That's a little over half the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'google/mt5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI: [here](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#processing-the-data) it says to specify `tokenizer.src_lang` and `tokenizer.tgt_lang` for multilingual models/tokeinzers, but this tokenizer does not have these properties."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the tokenizer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ds_train[0]['sentence1']\n",
    "s2 = ds_train[0]['sentence2']\n",
    "print(s1)\n",
    "print(s2)\n",
    "inputs = tokenizer(s1, text_target=s2)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a preprocess function that turns a dataset item into a form that the model can use for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find out what a reasonable `max_len` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_lengths = list(map(lambda x: max(len(x['sentence1']), len(x['sentence2'])), ds_train))\n",
    "val_lengths = list(map(lambda x: max(len(x['sentence1']), len(x['sentence2'])), ds_train))\n",
    "test_lengths = list(map(lambda x: max(len(x['sentence1']), len(x['sentence2'])), ds_train))\n",
    "\n",
    "plt.hist(train_lengths + val_lengths + test_lengths, 100)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the y axis here is characters and not tokens. With 128 (for `max_len`) we are on the safe side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "# the prefix has to (dynamically) be adjusted depending on the language or when training multilingually (I think).\n",
    "prefix = 'paraphrase: '\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix+s1 for s1 in examples['sentence1']]\n",
    "    targets = examples['sentence2']\n",
    "    # most likely there will be nothing to truncate, but we still add it\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the preprocessing function to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_train = ds_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds_train.column_names\n",
    ")\n",
    "tokenized_ds_val = ds_val.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds_val.column_names\n",
    ")\n",
    "tokenized_ds_test = ds_test.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=ds_test.column_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the model and a Datacollator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'll skip the example usage of the datacollator, check it out [here](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#data-collation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's continue with metrics. We will use Parascore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
